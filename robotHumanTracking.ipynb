{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "   \n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "   x = camera_value\n",
    "   x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "   x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "   x -= mean[:, None, None]\n",
    "   x /= stdev[:, None, None]\n",
    "   return x[None, ...]\n",
    "\n",
    "\n",
    "class ObjectDetector(object):\n",
    "   \n",
    "   def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "       logger = trt.Logger()\n",
    "       trt.init_libnvinfer_plugins(logger, '')\n",
    "       load_plugins()\n",
    "       self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "       self.preprocess_fn = preprocess_fn\n",
    "       \n",
    "   def execute(self, *inputs):\n",
    "       trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "       return parse_boxes(trt_outputs)\n",
    "   \n",
    "   def __call__(self, *inputs):\n",
    "       return self.execute(*inputs)\n",
    "\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "#pip install opencv-contrib-python --upgrade\n",
    "#Used for tracking the first \"label 1\" the camera sees\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    #this changing of this value will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolution for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "        \n",
    "        #start the RGBD sensor\n",
    "        self.pipeline.start(self.configuration)\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "\n",
    "        #start capture the first color image\n",
    "        color_frame = frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        #start capture the first depth image\n",
    "        depth_frame = frames.get_depth_frame()           \n",
    "        self.depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_colormap   \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            frames = self.pipeline.wait_for_frames() #receive data from RGBD sensor\n",
    "            \n",
    "            color_frame = frames.get_color_frame() #get the color image\n",
    "            image = np.asanyarray(color_frame.get_data()) #convert color image to numpy array\n",
    "            self.color_value = image #assign the numpy array image to the color_value variable \n",
    "\n",
    "            depth_frame = frames.get_depth_frame() #get the depth image           \n",
    "            self.depth_image = np.asanyarray(depth_frame.get_data()) #convert depth data to numpy array\n",
    "            #conver depth data to BGR image for displaying purpose\n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(self.depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap #assign the color BGR image to the depth value\n",
    "    \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread \n",
    "            \n",
    "    def getDepth(self,x,y):\n",
    "        if x > 640:\n",
    "            x = 620\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y > 480:\n",
    "            y = 460\n",
    "        if y < 0:\n",
    "            y = 0\n",
    "        return self.depth_image[int(x),int(y)]\n",
    "            \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget,]),\n",
    "    label_widget\n",
    "]))\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()\n",
    "\n",
    "# Initialize the tracker\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "tracker_initialized = False\n",
    "\n",
    "def processing(change):\n",
    "    global tracker_initialized, tracker\n",
    "\n",
    "    image = change['new']  # This is your camera frame input\n",
    "\n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    \n",
    "\n",
    "    if not tracker_initialized:\n",
    "        # compute all detected objects\n",
    "        detections = model(imgsized)\n",
    "\n",
    "        for det in detections:\n",
    "            # Assuming 'det' has the properties 'label', 'confidence', 'xmin', 'ymin', 'xmax', 'ymax'\n",
    "            if det['label'] == 1 and det['confidence'] > 0.5:\n",
    "                bbox = (det['xmin'], det['ymin'], det['xmax'] - det['xmin'], det['ymax'] - det['ymin'])\n",
    "                tracker.init(image, bbox)\n",
    "                tracker_initialized = True\n",
    "                break\n",
    "\n",
    "    if tracker_initialized:\n",
    "        success, bbox = tracker.update(image)  # Update the tracker\n",
    "        if success:\n",
    "            x, y, w, h = map(int, bbox)\n",
    "            center_x = x + w / 2.0\n",
    "            center_y = y + h / 2.0\n",
    "\n",
    "            # Determine the midpoint of the frame's width\n",
    "            frame_midpoint = image.shape[1] // 2\n",
    "            # Calculate 10% of the frame's width to define the center zone\n",
    "            center_zone_width = image.shape[1] * 0.1\n",
    "            left_center_zone = frame_midpoint - (center_zone_width / 2)\n",
    "            right_center_zone = frame_midpoint + (center_zone_width / 2)\n",
    "\n",
    "            # Check if the center point is near the left or right edge or has moved off-frame\n",
    "            if center_x < 10:\n",
    "                print(\"Action: Near left edge or moved off-frame to the left\")\n",
    "                robot.left(0.5)  # Turn left\n",
    "            elif center_x > image.shape[1] - 10:\n",
    "                print(\"Action: Near right edge or moved off-frame to the right\")\n",
    "                robot.right(0.5)  # Turn right\n",
    "\n",
    "            # Check if the center point is within the center zone\n",
    "            elif left_center_zone <= center_x <= right_center_zone:\n",
    "                print(\"Action: In the center zone of the frame\")\n",
    "                robot.forward(0.3)  # Move forward if roughly centered\n",
    "            elif center_x < frame_midpoint:\n",
    "                print(\"Action: In the left half of the frame\")\n",
    "                robot.forwardleft(0.5)  # Turn left\n",
    "            elif center_x > frame_midpoint:\n",
    "                print(\"Action: In the right half of the frame\")\n",
    "                robot.forwardright(0.5)  # Turn right\n",
    "\n",
    "            # Draw the updated bounding box and center point\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            cv2.circle(image, (center_x, center_y), 5, (0, 255, 0), -1)  # Draw center point\n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "camera.observe(processing, names='color_value')  # Call 'processing' on each new frame"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
